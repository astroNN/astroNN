{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Third-party packages\n",
    "import h5py\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# this package\n",
    "from astronn.data import fetch_notMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache_file = fetch_notMNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_map = list('abcdefghij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axes = pl.subplots(3,3,figsize=(5,5),sharex=True,sharey=True)\n",
    "with h5py.File(cache_file, 'r') as f:\n",
    "    for i in range(9):\n",
    "        ax = axes.flat[i]\n",
    "        \n",
    "        idx = np.random.randint(f['test']['images'].shape[0])\n",
    "        ax.imshow(f['test']['images'][idx],\n",
    "                  cmap='Greys', interpolation='nearest')\n",
    "        ax.set_title(label_map[int(f['test']['labels'][idx])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Mean images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution: \n",
    "with h5py.File(cache_file, 'r') as f:\n",
    "    # get a unique list of the classes\n",
    "    classes = np.unique(f['test']['labels'])\n",
    "    classes.sort()\n",
    "    nclasses = len(classes)\n",
    "    \n",
    "    images = f['test']['images'][:]\n",
    "    for i,cls in enumerate(classes):\n",
    "        fig,ax = pl.subplots(1,1,figsize=(2,2))\n",
    "        mean_img = images[f['test']['labels'][:] == cls].mean(axis=0) # select all images for a given class, take mean\n",
    "        ax.imshow(mean_img, cmap='Greys', interpolation='nearest') # greyscale colormap, no interpolation\n",
    "        ax.set_title(label_map[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "Randomize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomize(data, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_data = data[permutation]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_data, shuffled_labels\n",
    "\n",
    "with h5py.File(cache_file, 'r') as f:\n",
    "    train_dataset, train_labels = randomize(f['train']['images'][:], f['train']['labels'][:])\n",
    "    test_dataset, test_labels = randomize(f['test']['images'][:], f['test']['labels'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "Number per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.histogram(train_labels, bins=np.arange(0,nclasses+1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so there are about 50000 in each class in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.histogram(test_labels, bins=np.arange(0,nclasses+1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And about 1870 in each class in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 5\n",
    "---------\n",
    "How much overlap is there between training, validation and test samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_overlaps = []\n",
    "# the data has been randomize, so let's just check the first 100 images and assume that\n",
    "#    is a representative sample\n",
    "for test_img in test_dataset[:100]: \n",
    "    diff = (train_dataset - test_img[None]).sum(axis=-1).sum(axis=-1)\n",
    "    n_overlap = (diff == 0).sum()\n",
    "    n_overlaps.append(n_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Typical overlap:\", np.median(n_overlaps))\n",
    "pl.hist(n_overlaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about near duplicates between datasets? (images that are almost identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_overlaps = []\n",
    "threshold = 1E-2 # define an arbitrary threshold -- play with this\n",
    "\n",
    "# the data has been randomize, so let's just check the first 100 images and assume that\n",
    "#    is a representative sample\n",
    "for test_img in test_dataset[:100]: \n",
    "    diff = (train_dataset - test_img[None]).sum(axis=-1).sum(axis=-1)\n",
    "    n_overlap = (np.abs(diff) < threshold).sum()\n",
    "    n_overlaps.append(n_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 6\n",
    "---------\n",
    "\n",
    "Train a logistic regressor on the image data using 50, 100, 1000 and 5000 training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = train_dataset.shape[-1]\n",
    "subset = 50 # replace with 100, 1000, 5000\n",
    "\n",
    "idx = np.random.choice(np.arange(train_dataset.shape[0]), size=subset)\n",
    "train_subset_data = train_dataset[idx].reshape(subset, image_size*image_size)\n",
    "train_subset_labels = train_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_subset_data, train_subset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_labels = model.predict(test_dataset.reshape(test_dataset.shape[0], image_size*image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(predict_labels != test_labels).sum() / float(test_labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}